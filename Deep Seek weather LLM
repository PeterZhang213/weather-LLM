# Install dependencies (uncomment if running first time)
# !pip install transformers accelerate requests

import os
import torch
import requests
import re
from transformers import AutoTokenizer, AutoModelForCausalLM

# ✅ Environment and Device Setup
os.environ["TOKENIZERS_PARALLELISM"] = "false"
device = "cuda" if torch.cuda.is_available() else "cpu"

# ✅ Load DeepSeek Model and Tokenizer
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", trust_remote_code=True).to(device)

# ✅ WeatherAPI Setup
API_KEY = "6e94eb2b9837487481d184826250401"
WEATHER_API_URL = "http://api.weatherapi.com/v1/forecast.json"

def get_weather_info(city):
    try:
        params = {
            "key": API_KEY,
            "q": city,
            "days": 1,
            "aqi": "no",
            "alerts": "no"
        }
        response = requests.get(WEATHER_API_URL, params=params)
        data = response.json()
        temp_c = data["current"]["temp_c"]
        condition = data["current"]["condition"]["text"]
        return temp_c, condition
    except Exception as e:
        print("Weather API Error:", e)
        return 22, "Clear"  # Fallback default values

def is_chinese(text):
    """Check if the question is mainly in Chinese."""
    return any('\u4e00' <= char <= '\u9fff' for char in text)

def generate_prompt(city, question):
    temp_c, condition = get_weather_info(city)
    if is_chinese(question):
        prompt = (
            f"你是一个乐于助人的天气助手。\n"
            f"{city}当前天气：{temp_c}°C，{condition}。\n"
            f"用户提问：“{question}”\n"
            f"请认真思考，并将你的最终建议用<answer>和</answer>括起来。"
        )
    else:
        prompt = (
            f"You are a helpful weather assistant.\n"
            f"Current weather in {city}: {temp_c}°C and {condition.lower()}.\n"
            f"User asks: \"{question}\"\n"
            f"Please think carefully and enclose your final advice between <answer> and </answer>."
        )
    return prompt

def extract_answer(text):
    match = re.search(r"<answer>(.*?)</answer>", text, re.DOTALL)
    return match.group(1).strip() if match else "No answer found."

# ✅ User Input
city = "广州"  # Can be English or Chinese city name
question = "今天适合穿T恤吗？"  # Try also: "Should I bring an umbrella?"

# ✅ Generate Prompt
prompt = generate_prompt(city, question)
print("Prompt:\n", prompt)

# ✅ Tokenize and Generate Response
inputs = tokenizer(prompt, return_tensors="pt").to(device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=300,
        do_sample=True,
        temperature=0.7
    )

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# ✅ Show Full Output and Extracted Answer
print("\nGenerated Text:\n", generated_text)
print("\nExtracted Answer:\n", extract_answer(generated_text))
